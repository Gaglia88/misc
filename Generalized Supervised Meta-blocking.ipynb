{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "printable-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-corporation",
   "metadata": {},
   "source": [
    "# Generalized supervised meta-blocking\n",
    "Generalized Supervised Meta-blocking employs the probability provided by a probabilistic classifier to score the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-leone",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First, load a clean dataset with the groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bulgarian-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"datasets/DblpAcm\"\n",
    "\n",
    "profiles1 = sparker.JSONWrapper.load_profiles('/data2/luca/ER/'+dataset+'/dataset1.json', \n",
    "                                              real_id_field = \"realProfileID\",\n",
    "                                              source_id=1)\n",
    "separator_id = profiles1.map(lambda profile: profile.profile_id).max()\n",
    "separator_ids = [separator_id]\n",
    "\n",
    "profiles2 = sparker.JSONWrapper.load_profiles('/data2/luca/ER/'+dataset+'/dataset2.json', \n",
    "                                              start_id_from = separator_id+1, \n",
    "                                              real_id_field = \"realProfileID\",\n",
    "                                              source_id=2)\n",
    "max_profile_id = profiles2.map(lambda profile: profile.profile_id).max()\n",
    "profiles = profiles1.union(profiles2)\n",
    "\n",
    "gt = sparker.JSONWrapper.load_groundtruth('/data2/luca/ER/'+dataset+'/groundtruth.json', 'id1', 'id2')\n",
    "new_gt = sparker.Converters.convert_groundtruth(gt, profiles1, profiles2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-shakespeare",
   "metadata": {},
   "source": [
    "## Blocking \n",
    "\n",
    "Performs the blocking by using standard token blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "established-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = sparker.Blocking.create_blocks(profiles, separator_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-abortion",
   "metadata": {},
   "source": [
    "## Block cleaning\n",
    "Applying some block cleaning techniques to remove some superfluous comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "geographic-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfoms the purging\n",
    "blocks_purged = sparker.BlockPurging.block_purging(blocks, 1.025)\n",
    "# Performs the cleaning\n",
    "(profile_blocks, profile_blocks_filtered, blocks_after_filtering) = sparker.BlockFiltering.\\\n",
    "                                                                            block_filtering_quick(blocks_purged, \n",
    "                                                                                                  0.8, \n",
    "                                                                                                  separator_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-matthew",
   "metadata": {},
   "source": [
    "## Features generation\n",
    "Generate the features set for each pair of entity profiles that co-occurs in at least one block (i.e. the edges of the meta-blocking graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aggregate-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sparker.FeatureGenerator.generate_features(profiles, blocks_after_filtering, separator_ids, new_gt, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handed-constraint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+------------+---------+---------+-----------+----------+-----------+------------+--------+\n",
      "| p1|  p2|    cfibf|       raccb|          js|numCompP1|numCompP2|         rs|      aejs|        nrs|         wjs|is_match|\n",
      "+---+----+---------+------------+------------+---------+---------+-----------+----------+-----------+------------+--------+\n",
      "|  0|2816|41.659367|0.0034722222|0.0035714286|      143|       90|0.029411765|0.99048966|0.014045067|0.0017414601|       0|\n",
      "|  0|4167|43.542595|0.0034722222|0.0038610038|      143|       87|0.029411765| 1.1286342|0.013155922|0.0010594048|       0|\n",
      "|  0|3399| 41.13539|0.0034722222| 0.003021148|      143|      105|0.029411765|0.79306006|0.020184455|0.0025181086|       0|\n",
      "|  0|3340|41.659367|0.0034722222|0.0041841003|      143|       66|0.029411765| 1.1582178|0.009341111| 7.561535E-4|       0|\n",
      "|  0|4241| 45.18775|0.0034722222| 0.004672897|      143|       43|0.029411765| 1.4860764|0.017673213| 0.002070604|       0|\n",
      "|  0|4242|42.852882|0.0034722222| 0.004132231|      143|       68|0.029411765| 1.0862926|0.012133722|0.0010303553|       0|\n",
      "|  0|3091| 44.31363|0.0034722222| 0.003937008|      143|       85|0.029411765| 1.2449707|0.016048362|0.0014678818|       0|\n",
      "|  0|3412|42.852882|0.0034722222| 0.002967359|      143|      136|0.029411765|0.93935233|0.025409684|0.0030547732|       0|\n",
      "|  0|3487|42.852882|0.0034722222|0.0038022813|      143|       85|0.029411765| 1.1608019| 0.01575486|0.0015235882|       0|\n",
      "|  0|3744|40.198624|0.0034722222|0.0032573289|      143|       97|0.029411765| 0.8036708|0.014400877|0.0020670476|       0|\n",
      "+---+----+---------+------------+------------+---------+---------+-----------+----------+-----------+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-hygiene",
   "metadata": {},
   "source": [
    "## Scoring the edges\n",
    "By using a probabilistic classifier (logistic regression) we assign to each pair (edge of the meta-blocking graph) the probability of being a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "moving-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import FloatType, BooleanType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-radar",
   "metadata": {},
   "source": [
    "### Generation of the feature vector for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tutorial-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to employ\n",
    "features_set = [\"cfibf\", \"raccb\", \"js\", \"numCompP1\", \"numCompP2\", \"rs\", \"aejs\", \"nrs\", \"wjs\"]\n",
    "\n",
    "va = VectorAssembler(inputCols=features_set, outputCol=\"features\")\n",
    "\n",
    "df = va.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-floor",
   "metadata": {},
   "source": [
    "### Split the data in train/test\n",
    "This will generate a balanced training set in which the number of positive/negative samples is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "structured-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per class (actually spark do not ensure this exact value during sampling)\n",
    "n_samples = 20\n",
    "\n",
    "# Sampling of matching pairs\n",
    "matches = df.where(\"is_match == 1\")\n",
    "m_n = n_samples/matches.count()\n",
    "m_train, m_test = matches.randomSplit([m_n, 1-m_n])\n",
    "\n",
    "# Sampling of non-matching pairs\n",
    "non_matches = df.where(\"is_match == 0\")\n",
    "nm_n = n_samples/non_matches.count()\n",
    "nm_train, nm_test = non_matches.randomSplit([nm_n, 1-nm_n])\n",
    "\n",
    "# Train/Test\n",
    "train = m_train.union(nm_train)\n",
    "test = m_test.union(nm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-nebraska",
   "metadata": {},
   "source": [
    "### Training the classifier and get the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rotary-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', \n",
    "                        labelCol='is_match', \n",
    "                        predictionCol='prediction', \n",
    "                        maxIter=1000, \n",
    "                        probabilityCol='probability'\n",
    "                       )\n",
    "# Training\n",
    "model = lr.fit(train)\n",
    "# Performs the predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Get the results as the probability of each pair (edge) of being a match\n",
    "get_p_match = f.udf(lambda v: float(v[1]), FloatType())\n",
    "edges = predictions\\\n",
    "        .withColumn(\"p_match\", get_p_match(\"probability\"))\\\n",
    "        .select(\"p1\", \"p2\", \"p_match\", \"is_match\")\n",
    "\n",
    "#edges.cache()\n",
    "#edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-layout",
   "metadata": {},
   "source": [
    "## Perform the pruning\n",
    "Perform the pruning and get the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "standard-eclipse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.9680755395683454\n",
      "Precision 0.6123435722411832\n",
      "F1 0.7501742160278746\n"
     ]
    }
   ],
   "source": [
    "pruned_edges = sparker.SupervisedMB.wep(edges)\n",
    "pc, pq, f1 = sparker.SupervisedMB.get_stats(pruned_edges, new_gt)\n",
    "print(\"Recall \"+str(pc))\n",
    "print(\"Precision \"+str(pq))\n",
    "print(\"F1 \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "saved-allocation",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\n    return lambda *a: f(*a)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/data2/luca/gnn/features/sparker/supervised_metablocking.py\", line 35, in do_pruning\n    threshold = 0.35 * (profiles1_max_proba.value[p1] + profiles2_max_proba.value[p2])\nKeyError: 753\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f766e6d23e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpruned_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupervisedMB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupervisedMB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruned_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/luca/gnn/features/sparker/supervised_metablocking.py\u001b[0m in \u001b[0;36mget_stats\u001b[0;34m(edges, groundtruth)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mgt_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mnum_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"is_match == 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mnum_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\n    return lambda *a: f(*a)\n  File \"/data2/luca/spark/spark-3.0.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"/data2/luca/gnn/features/sparker/supervised_metablocking.py\", line 35, in do_pruning\n    threshold = 0.35 * (profiles1_max_proba.value[p1] + profiles2_max_proba.value[p2])\nKeyError: 753\n"
     ]
    }
   ],
   "source": [
    "pruned_edges = sparker.SupervisedMB.blast(edges)\n",
    "pc, pq, f1 = sparker.SupervisedMB.get_stats(pruned_edges, new_gt)\n",
    "print(\"Recall \"+str(pc))\n",
    "print(\"Precision \"+str(pq))\n",
    "print(\"F1 \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_edges = sparker.SupervisedMB.rcnp(edges, profiles, blocks)\n",
    "pc, pq, f1 = sparker.SupervisedMB.get_stats(pruned_edges, new_gt)\n",
    "print(\"Recall \"+str(pc))\n",
    "print(\"Precision \"+str(pq))\n",
    "print(\"F1 \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_edges = sparker.SupervisedMB.cnp(edges, profiles, blocks)\n",
    "pc, pq, f1 = sparker.SupervisedMB.get_stats(pruned_edges, new_gt)\n",
    "print(\"Recall \"+str(pc))\n",
    "print(\"Precision \"+str(pq))\n",
    "print(\"F1 \"+str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-century",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
