{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accessory-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import FloatType, BooleanType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-essay",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-sterling",
   "metadata": {},
   "source": [
    "The goal is to perfom entity resolution, i.e. identifying if records are true matches.\n",
    "\n",
    "The dataframe represents a set of edges which connect different records (profiles).\n",
    "\n",
    "The goal is to keep only the most relevant edges for each profile (prune the graph).\n",
    "\n",
    "Each edge has a set of features, by using these features the probability of being a true match (*p_match*) is assigned to each edge.\n",
    "\n",
    "*is_match* come from a groundtruth and is used to compute recall/precision of the method employed to prune the graph.\n",
    "\n",
    "\n",
    "Records comes from two different data sources, so p1 are the identifiers of the records from source 1, so p2 are the identifiers of the records from source 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "military-summit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+------------+---------+---------+-----------+---------+------------+-----------+--------+\n",
      "| p1|  p2|    cfibf|       raccb|          js|numCompP1|numCompP2|         rs|     aejs|         nrs|        wjs|is_match|\n",
      "+---+----+---------+------------+------------+---------+---------+-----------+---------+------------+-----------+--------+\n",
      "|  0|3461| 41.13539|0.0019762847| 0.004032258|      143|       74|0.022222223|1.0641375|0.0075938664|5.493571E-4|       0|\n",
      "|  0|3336|39.005108|0.0019762847|0.0028818443|      143|      109|0.022222223|0.7368643| 0.009265238|9.440924E-4|       0|\n",
      "+---+----+---------+------------+------------+---------+---------+-----------+---------+------------+-----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = spark.read.option('header', 'true').option(\"inferSchema\",\"true\").csv('features.csv')\n",
    "features.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-import",
   "metadata": {},
   "source": [
    "## Scoring the edges\n",
    "By using a probabilistic classifier (logistic regression) we assign to each pair (edge of the meta-blocking graph) the probability of being a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indie-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to employ\n",
    "features_set = [\"cfibf\", \"raccb\", \"js\", \"numCompP1\", \"numCompP2\", \"rs\", \"aejs\", \"nrs\", \"wjs\"]\n",
    "\n",
    "va = VectorAssembler(inputCols=features_set, outputCol=\"features\")\n",
    "\n",
    "df = va.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-memory",
   "metadata": {},
   "source": [
    "### Split the data in train/test\n",
    "This will generate a balanced training set in which the number of positive/negative samples is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per class (actually spark do not ensure this exact value during sampling)\n",
    "n_samples = 20\n",
    "\n",
    "# Sampling of matching pairs\n",
    "matches = df.where(\"is_match == 1\")\n",
    "m_n = n_samples/matches.count()\n",
    "m_train, m_test = matches.randomSplit([m_n, 1-m_n])\n",
    "\n",
    "# Sampling of non-matching pairs\n",
    "non_matches = df.where(\"is_match == 0\")\n",
    "nm_n = n_samples/non_matches.count()\n",
    "nm_train, nm_test = non_matches.randomSplit([nm_n, 1-nm_n])\n",
    "\n",
    "# Train/Test\n",
    "train = m_train.union(nm_train)\n",
    "test = m_test.union(nm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-christianity",
   "metadata": {},
   "source": [
    "### Training the classifier and get the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vanilla-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', \n",
    "                        labelCol='is_match', \n",
    "                        predictionCol='prediction', \n",
    "                        maxIter=1000, \n",
    "                        probabilityCol='probability'\n",
    "                       )\n",
    "# Training\n",
    "model = lr.fit(train)\n",
    "# Performs the predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Get the results as the probability of each pair (edge) of being a match\n",
    "get_p_match = f.udf(lambda v: float(v[1]), FloatType())\n",
    "edges = predictions\\\n",
    "        .withColumn(\"p_match\", get_p_match(\"probability\"))\\\n",
    "        .select(\"p1\", \"p2\", \"p_match\", \"is_match\")\n",
    "\n",
    "#edges.cache()\n",
    "#edges.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-jungle",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-andrews",
   "metadata": {},
   "source": [
    "First, keep only the edges with a score greater than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "signal-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_t = edges.filter(\"p_match >= 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-custody",
   "metadata": {},
   "source": [
    "For each record of source 1 create a hashmap that given the record id returns the maximum weight associated to its connected edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "directed-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles1_max_proba = sc.broadcast(over_t.groupby('p1').max('p_match').rdd.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-competition",
   "metadata": {},
   "source": [
    "Same for the records from source 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "balanced-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles2_max_proba = sc.broadcast(over_t.groupby('p2').max('p_match').rdd.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-faith",
   "metadata": {},
   "source": [
    "Prune the graph by combinig the maximum weights of each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sonic-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pruning(p1, p2, p_match):\n",
    "    threshold = 0.35 * (profiles1_max_proba.value[p1] + profiles2_max_proba.value[p2])\n",
    "    return p_match >= threshold\n",
    "\n",
    "pruning_udf = f.udf(do_pruning, BooleanType())\n",
    "\n",
    "res = over_t \\\n",
    "    .select(\"p1\", \"p2\", \"p_match\", \"is_match\", pruning_udf(\"p1\", \"p2\", \"p_match\").alias(\"keep\")) \\\n",
    "    .where(\"keep\") \\\n",
    "    .select(\"p1\", \"p2\", \"p_match\", \"is_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "european-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------+\n",
      "| p1|  p2|p_match|is_match|\n",
      "+---+----+-------+--------+\n",
      "|  0|2733|    1.0|       1|\n",
      "|  4|4066|    1.0|       1|\n",
      "|  8|4375|    1.0|       1|\n",
      "| 12|3829|    1.0|       1|\n",
      "| 16|2968|    1.0|       1|\n",
      "| 20|3358|    1.0|       1|\n",
      "| 24|2775|    1.0|       1|\n",
      "| 28|3792|    1.0|       1|\n",
      "| 32|3560|    1.0|       1|\n",
      "| 40|4174|    1.0|       1|\n",
      "| 44|3022|    1.0|       1|\n",
      "| 48|2976|    1.0|       1|\n",
      "| 52|4296|    1.0|       1|\n",
      "| 56|2879|    1.0|       1|\n",
      "| 60|3448|    1.0|       1|\n",
      "| 64|3872|    1.0|       1|\n",
      "| 68|4664|    1.0|       1|\n",
      "| 72|3703|    1.0|       1|\n",
      "| 76|3825|    1.0|       1|\n",
      "| 80|4077|    1.0|       1|\n",
      "+---+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-onion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
